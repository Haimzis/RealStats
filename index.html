<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RealStats: Real-Only Statistical Framework for Fake Image Detection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="icon" type="image/png" href="assets/SUFT_logo_small.png">
    <style>
        * {
          margin: 0;
          padding: 0;
          box-sizing: border-box;
        }
        
        body {
          font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
          line-height: 1.6;
          color: #1a1a1a;
          background: #f8faff;
        }
        
        .container {
          max-width: 1200px;
          margin: 0 auto;
          padding: 0 20px;
        }

        .links_footer {
          max-width: 1200px;
          margin: 0 auto;
          padding: 0 20px;
        }

        .links_footer a {
            color: inherit;
            text-decoration: none;
        }
        
        /* Header */
        header {
          background: linear-gradient(135deg, #232d5d 0%, #4b9eff 100%);
          color: white;
          padding: 80px 0 60px;
          text-align: center;
        }
        
        h1 {
          font-size: 2.8em;
          margin-bottom: 20px;
          font-weight: 700;
        }
        
        .venue {
          font-size: 1.3em;
          margin-bottom: 30px;
          color: #e8f0ff;
          font-weight: 600;
          letter-spacing: 0.5px;
          text-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
          opacity: 0.95;
        }
        
        .authors {
          margin-bottom: 15px;
          line-height: 1.8;
          color: #f0f5ff;
        }
        .authors a {
          font-size: 1.2em;
          text-decoration: none;
          font-weight: 400;
          color: #f0f5ff;
        }
        
        .affiliations {
          font-size: 1.1em;
          opacity: 0.9;
          margin-bottom: 30px;
          color: #dce7ff;
        }
        
        /* Links */
        .links {
          display: flex;
          justify-content: center;
          gap: 20px;
          flex-wrap: wrap;
          margin-top: 30px;
        }
        
        .link-button {
          display: inline-flex;
          align-items: center;
          gap: 8px;
          padding: 12px 28px;
          background: white;
          color: #232d5d;
          text-decoration: none;
          border-radius: 50px;
          transition: all 0.3s;
          border: 1px solid #0866ff;
          font-weight: 500;
        }
        
        .link-button:hover {
          background: linear-gradient(135deg, #0866ff 0%, #4b9eff 100%);
          color: white;
          transform: translateY(-2px);
          box-shadow: 0 4px 15px rgba(8, 102, 255, 0.3);
        }
        
        /* Main */
        main {
          background: white;
          margin: -30px auto 40px;
          border-radius: 20px;
          box-shadow: 0 8px 30px rgba(0, 0, 0, 0.08);
          overflow: hidden;
        }
        
        section {
          padding: 60px 80px;
          border-bottom: 1px solid #e3edff;
        }
        
        section:last-child {
          border-bottom: none;
        }
        
        h2 {
          font-size: 2em;
          margin-bottom: 30px;
          color: #232d5d;
          text-align: center;
          font-weight: 600;
        }
        
        h3 {
          font-size: 1.4em;
          margin: 30px 0 15px;
          color: #232d5d;
        }
        
        /* Abstract */
        .abstract {
          background: linear-gradient(135deg, #f2f7ff 0%, #e6f0ff 100%);
        }
        
        .abstract p {
          font-size: 1.1em;
          text-align: justify;
          line-height: 1.8;
          color: #333;
        }
        
        /* Figures */
        .figure {
          margin: 40px 0;
          text-align: center;
        }

        .figure img {
          max-width: 100%;
          height: auto;
          border-radius: 10px;
          box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
        }

        .figure-caption {
          margin-top: 15px;
          font-size: 0.95em;
          color: #5679b5;
          font-style: italic;
        }

        .figure-row {
          display: grid;
          grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
          gap: 20px;
          align-items: start;
        }
        
        /* Method Overview */
        .method-grid {
          display: grid;
          grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
          gap: 30px;
          margin: 30px 0;
        }
        
        .method-card {
          background: #f8fbff;
          padding: 25px;
          border-radius: 12px;
          border-left: 4px solid #232d5d;
          box-shadow: 0 3px 10px rgba(0, 0, 0, 0.05);
        }
        
        .method-card h4 {
          color: #232d5d;
          margin-bottom: 10px;
          font-size: 1.2em;
        }
        
        /* Results */
        .results-grid {
          display: grid;
          grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
          gap: 25px;
          margin: 30px 0;
        }
        
        .result-card {
          background: linear-gradient(135deg, #232d5d 0%, #4b9eff 100%);
          color: white;
          padding: 30px;
          border-radius: 12px;
          text-align: center;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
        }
        
        .result-card .metric {
          font-size: 2.5em;
          font-weight: 700;
          margin-bottom: 10px;
        }
        
        .result-card .label {
          font-size: 1em;
          opacity: 0.9;
        }
        
        /* Citation */
        .citation-box {
          background: #f8fbff;
          border-left: 4px solid #232d5d;
          padding: 20px;
          margin: 30px 0;
          font-family: 'Courier New', monospace;
          font-size: 0.9em;
          overflow-x: auto;
          box-shadow: 0 3px 10px rgba(0, 0, 0, 0.05);
        }
        
        /* Responsive */
        @media (max-width: 768px) {
          h1 { font-size: 2em; }
          section { padding: 40px 30px; }
          .links { flex-direction: column; align-items: center; }
          .method-grid, .results-grid { grid-template-columns: 1fr; }
          .abstract p { text-align: initial; }
        }
        
        /* Placeholder */
        .placeholder-image {
          background: linear-gradient(135deg, #e6f0ff 0%, #f4f8ff 100%);
          border: 2px dashed #a7c6ff;
          display: flex;
          align-items: center;
          justify-content: center;
          min-height: 300px;
          border-radius: 10px;
          color: #4b78d0;
          font-size: 1.2em;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>RealStats: Real-Only Statistical Framework for Fake Image Detection</h1>
            <div class="venue">Preprint · Target: AISTATS 2026</div>
            <div class="authors">
                <strong><a href="https://www.linkedin.com/in/haim-zisman/" target="_blank">Haim Zisman</a></strong>,
                <strong><a href="https://www.linkedin.com/in/urishaham/" target="_blank">Uri Shaham</a></strong>
            </div>
            <div class="affiliations">
                Bar-Ilan University · Shaham Lab
            </div>
            <div class="links">
                <a href="#" class="link-button" target="_blank">
                    <i class="fas fa-file-pdf"></i> Paper (Soon)
                </a>
                <a href="supplementary.tex" class="link-button" target="_blank">
                    <i class="fas fa-layer-group"></i> Supplementary
                </a>
                <a href="#" class="link-button" target="_blank">
                    <i class="fas fa-link"></i> OpenReview (Soon)
                </a>
                <a href="https://github.com/shaham-lab/RealStats" class="link-button" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="#" class="link-button" id="copy-bibtex-btn" target="_blank">
                    <i class="fas fa-quote-left"></i> BibTeX
                </a>
                <span id="bibtex-notification" style="display:none; margin-left:10px; color:#000000; background:#EFBF04; border-radius:20px; font-size:0.95em;">BibTeX Copied!</span>
                <script>
                    document.addEventListener('DOMContentLoaded', function() {
                        const btn = document.getElementById('copy-bibtex-btn');
                        const notif = document.getElementById('bibtex-notification');
                        btn.addEventListener('click', function(e) {
                            e.preventDefault();
                            const bibtex = `@inproceedings{zisman2026realstats,
                                                title={RealStats: A Real-Only Statistical Framework for Fake Image Detection},
                                                author={Haim Zisman and Uri Shaham},
                                                booktitle={AISTATS},
                                                year={2026},
                                                url={https://github.com/shaham-lab/RealStats}
                                            }`;
                            navigator.clipboard.writeText(bibtex).then(function() {
                                notif.style.display = 'inline-block';
                                setTimeout(function() {
                                    notif.style.display = 'none';
                                }, 1500);
                            });
                        });
                    });
                </script>
            </div>
        </div>
    </header>

    <main class="container">
        <!-- Abstract -->
        <section class="abstract">
          <h2>Abstract</h2>
          <p>
            As generative models continue to advance, detecting AI-generated images remains a significant challenge. Existing detection methods can be effective, but they often lack formal <b>interpretability</b> and rely on implicit assumptions about fake content, which may limit their <b>adaptability</b> under distribution shifts. In this work, we introduce <b>RealStats</b>, a statistically rigorous, training-free framework for fake-image detection that produces probability scores interpretable with respect to the real-image population. RealStats leverages the strengths of multiple existing detectors by combining strong, training-free statistical measures. Specifically, we compute <em>p</em>-values across a range of test statistics and aggregate them using classical statistical ensembling to assess how well an image aligns with a unified real-image distribution. This framework is generic, flexible, and entirely training-free, making it well-suited for robust fake-image detection in diverse and continually evolving settings.
        </p>
        </section>

        <!-- Results -->
        <section>
            <h2>Interpretability Does Not Come at the Price of Performance</h2>

            <div class="figure-row">
              <div class="figure">
                <img src="assets/images/fig5/all_auc_bar_chart.pdf" alt="Bar chart comparing average AUC and AP against training-free baselines." class="responsive-figure-img">
              </div>
              <div class="figure">
                <img src="assets/images/fig6/polygon_plot.pdf" alt="Radar plot comparing AUC per generator across methods." class="responsive-figure-img">
              </div>
            </div>
            <p class="figure-caption">
              A central goal of our method is to provide interpretable and reliable detection. Interpretability is a direct result of returning a <em>p</em>-value for each inference image, which is a value with clear statistical meaning rather than an ambiguous “realness score.” Crucially, we demonstrate that despite this design choice, our method achieves performance on par with state-of-the-art training-free detectors. As shown in Table 1, our method achieves competitive AUC and AP compared to state-of-the-art training-free baselines. While ManifoldBias and RIGID obtain slightly higher peak scores, our approach offers comparable performance with substantially lower variance across generators, reflecting more consistent behavior. Importantly, this competitiveness comes without sacrificing interpretability.
            </p>

            <h3>Statistical Grounding & Benchmark Highlights</h3>
            <div class="results-grid">
                <div class="result-card">
                    <div class="metric">Real-Only</div>
                    <div class="label">Hypothesis tests calibrated solely on real-image ECDFs—no synthetic training required.</div>
                </div>
                <div class="result-card">
                    <div class="metric">180K</div>
                    <div class="label">Images across CNNSpot, UFD, Stable Diffusion Face, Synthbuster, and GenImage.</div>
                </div>
                <div class="result-card">
                    <div class="metric">Competitive</div>
                    <div class="label">AUC and AP match training-free SOTA while keeping <em>p</em>-value interpretability.</div>
                </div>
                <div class="result-card">
                    <div class="metric">Stable</div>
                    <div class="label">Lower variance across generators than single-statistic baselines.</div>
                </div>
            </div>

            <p style="margin-top: 30px; font-size: 1.05em;">
              RealStats centers interpretability first: calibrated <em>p</em>-values quantify uncertainty with controlled false positives, while accuracy remains on par with leading training-free baselines across all evaluated generators.
            </p>
        </section>

        <!-- Method Overview -->
        <section>
          <h2>Overview of the RealStats Pipeline</h2>
          <p style="font-size: 1.05em; line-height: 1.8; text-align: justify;">
            RealStats operates in two phases. During <strong>null distribution modeling</strong>, we compute diverse training-free statistics on real images, estimate their empirical CDFs, and select an independent subset using chi-squared-based tests. At <strong>inference</strong>, incoming images are mapped to <em>p</em>-values through cached ECDFs and aggregated via Stouffer or Min-<em>p</em> rules, yielding interpretable decisions with provable error control.
          </p>
          <div class="figure">
            <img src="assets/images/fig3/calibration_no_background.png" alt="Null distribution modeling phase with ECDF estimation and independence graph extraction." class="responsive-figure-img">
            <p class="figure-caption">
              Null distribution modeling: statistics computed on real data are converted into ECDFs, pruned by independence tests, and arranged into a maximal clique that guarantees valid multi-test aggregation.
            </p>
          </div>
          <div class="figure">
            <img src="assets/images/fig4/inference.png" alt="Inference phase mapping statistics to p-values and aggregating them." class="responsive-figure-img">
            <p class="figure-caption">
              Inference: selected statistics are evaluated on a test image, transformed into <em>p</em>-values using stored ECDFs, and aggregated into a single interpretable probability of the image being real.
            </p>
          </div>
          <div class="method-grid">
            <div class="method-card">
                <h4><i class="fas fa-scale-balanced"></i> Formal Validity</h4>
                <p>Two-sided empirical <em>p</em>-values remain uniform under the null, ensuring controlled false positives when testing against real-image distributions.</p>
            </div>
            <div class="method-card">
                <h4><i class="fas fa-screwdriver-wrench"></i> Modular Statistics</h4>
                <p>Any scalar detector computed on real images can be added, ECDF-modeled, and aggregated without retraining.</p>
            </div>
            <div class="method-card">
                <h4><i class="fas fa-shuffle"></i> Adaptable Aggregation</h4>
                <p>Stouffer and Min-<em>p</em> ensembles hedge against failing statistics and retain power across emerging generators.</p>
            </div>
        </div>
        </section>

        <section>
            <h2>Adaptability in Action: Improving Performance on Challenging Generators</h2>
            <div class="figure">
                <img src="assets/images/fig8/adaptability_improvement.pdf" alt="Performance improvements after adding ManifoldBias statistic." class="responsive-figure-img">
                <p class="figure-caption">
                  5.2 Adaptability in Action: Improving Performance on Challenging Generators<br><br>
                  While our method performs on par with state-of-the-art training-free detectors overall, we observe weaker results on certain generators where ManifoldBias excels, including GauGAN, CycleGAN, and SAN (see Figure 5). To demonstrate adaptability, we revisit these cases and integrate the ManifoldBias statistic under the same experimental setup.<br><br>
                  As shown in Figure 6, the ensemble gains clear improvements on these generators, closing the gap with baselines by leveraging the strength of ManifoldBias.<br><br>
                  To highlight the broader impact, the benefit extends to our full benchmark of 180K images, where incorporating ManifoldBias improves p-value separation (Figure 7) and raises overall AUC scores. These results demonstrate the modularity and adaptability of our framework, showing it can respond to evolving generative content by selectively integrating new, independent statistics when needed.
                </p>
            </div>
        </section>

        <section>
            <h2>Statistical Significance at a Glance</h2>
            <div class="figure">
                <img src="assets/images/pvalues_distribution/significance_grid.png" alt="Grid of p-value significance across detectors and datasets." class="responsive-figure-img">
                <p class="figure-caption">
                    We further illustrate this interpretability by showing p-value distributions on our test data (Figure 12). Each row corresponds to a different p-value interval, with images annotated by ground-truth labels. Fake images tend to cluster at lower p-values, though some appear at higher values, indicating statistical similarity to real images. This visualization highlights how p-values provide a calibrated and interpretable measure of sample quality.
                </p>
            </div>
            <p style="margin-top: 30px; font-size: 1.05em; line-height: 1.8;">
                Integrating additional statistics such as <em>ManifoldBias</em> further improves separation on challenging generators (e.g., GauGAN, CycleGAN, SAN), illustrating the framework’s plug-and-play adaptability.
            </p>
        </section>

        <section>
            <h2>Interpretability & Adaptability in Our Experiments</h2>
            <div class="method-grid">
                <div class="method-card">
                    <h4><i class="fas fa-eye"></i> Interpretability</h4>
                    <p>Uniform real-sample <em>p</em>-values and calibrated thresholds (Fig. p-values grid) let practitioners audit error rates and diagnose uncertain cases without opaque logits.</p>
                </div>
                <div class="method-card">
                    <h4><i class="fas fa-seedling"></i> Adaptability</h4>
                    <p>Adding statistics like <em>ManifoldBias</em> or texture features immediately lifts performance on tougher generators (e.g., GauGAN, SAN) with no retraining.</p>
                </div>
                <div class="method-card">
                    <h4><i class="fas fa-chart-line"></i> Stable Power</h4>
                    <p>Across the 180K-image benchmark, both Stouffer and Min-<em>p</em> ensembles maintain AUC while avoiding the variance spikes seen in single-statistic baselines.</p>
                </div>
            </div>
        </section>

        <!-- Conclusion -->
        <section class="abstract">
            <h2>Conclusion</h2>
            <p>
                RealStats delivers interpretable, training-free fake-image detection grounded solely in real-image statistics. By estimating ECDFs on real data, enforcing independence, and aggregating calibrated <em>p</em>-values, the framework attains state-of-the-art training-free performance (0.775 AUC, 0.756 AP) across a 180K-image benchmark while remaining modular to new detectors and distribution shifts.

            </p>
        </section>
        <!-- Citation -->
        <section>
            <h2>Citation</h2>
            <div class="citation-box">
              @inproceedings{zisman2026realstats,<br>
              &nbsp;&nbsp;title={RealStats: A Real-Only Statistical Framework for Fake Image Detection},<br>
              &nbsp;&nbsp;author={Haim Zisman and Uri Shaham},<br>
              &nbsp;&nbsp;booktitle={International Conference on Artificial Intelligence and Statistics},<br>
              &nbsp;&nbsp;year={2026},<br>
              &nbsp;&nbsp;url={https://github.com/shaham-lab/RealStats}<br>
              }
            </div>
        </section>
        <!-- QR Codes -->
        <section id="qr-codes" class="qr-desktop-only">
          <h2>Scan the QR Codes</h2>
          <div>
            <img
              src="assets/qr_codes.png"
              alt="QR codes linking to the Project Page, Paper, and LinkedIn"
              class="responsive-figure-img"
              style="max-width: 900px; width: 100%; height: auto;"
            >
            <p class="figure-caption" style="text-align: center;">
              Quick links: Project Page · Paper · LinkedIn
            </p>
          </div>
        </section>

        <style>
          /* Hide QR section on small screens */
          @media (max-width: 768px) {
            .qr-desktop-only {
              display: none;
            }
          }
        </style>
    </main>

    <footer style="text-align: center; padding: 40px 0; color: #6c757d;">
        <div class="links_footer">
            <a href="https://www.linkedin.com/in/haim-zisman/" target="_blank">Haim Zisman</a> ·
            <a href="https://www.linkedin.com/in/urishaham/" target="_blank">Uri Shaham</a> ·
            <a href="https://aistats.org" target="_blank">AISTATS</a>
        </div>
    </footer>
</body>
</html>
